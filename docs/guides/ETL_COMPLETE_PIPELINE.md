# ðŸš€ Torre Control - ETL Pipeline Completo

## Arquitectura End-to-End

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FASE 1: INGESTIÃ“N (scripts/load_data.py)                            â”‚
â”‚ CSV Raw â†’ PostgreSQL Staging (stg_raw_orders)                        â”‚
â”‚ âœ… Carga 100K+ Ã³rdenes en 10-20 segundos                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FASE 2: TRANSFORMACIÃ“N (scripts/transform_data.py) â­ NUEVA         â”‚
â”‚ Staging â†’ Star Schema (dims + facts)                                 â”‚
â”‚ âœ… Crea 4 dimensiones + 1 fact con KPIs calculados                 â”‚
â”‚ âœ… Validaciones crÃ­ticas (NULLs, outliers, FK integrity)           â”‚
â”‚ âœ… AuditorÃ­a con etl_run_id (UUID)                                  â”‚
â”‚ â±ï¸  Tiempo: ~180-200 segundos                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FASE 3: EXPORTACIÃ“N (src/etl/export_star_schema.py)                 â”‚
â”‚ PostgreSQL Star Schema â†’ CSVs (Data/Processed/)                      â”‚
â”‚ âœ… Genera 5 archivos: fact_orders + 4 dims                          â”‚
â”‚ âœ… Formato: UTF-8, Ã­ndices, 50MB+ total                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FASE 4: VALIDACIÃ“N (scripts/load_data.py --validate)                â”‚
â”‚ ValidaciÃ³n de calidad de datos                                       â”‚
â”‚ âœ… Row counts, nulls, OTIF%, Revenue at Risk                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FASE 5: BUSINESS INTELLIGENCE (PBIX/)                               â”‚
â”‚ Power BI â†’ Dashboards Ejecutivos                                    â”‚
â”‚ âœ… 5 vistas: OTIF, Revenue Risk, Churn, Geography, Fraud           â”‚
â”‚ âœ… Drill-down: Market â†’ Region â†’ State â†’ City                       â”‚
â”‚ âœ… Real-time slicers: Date, Segment, Product                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸŽ¯ Fase 2: TransformaciÃ³n (Lo Nuevo)

### Â¿QuÃ© hace transform_data.py?

**Input:** `dw.stg_raw_orders` (tabla cruda con 54 campos)  
**Output:** 5 tablas optimizadas + 6 KPIs calculados  
**Tiempo:** ~180-200 segundos  
**Filas procesadas:** 186,523 Ã³rdenes  

### El Proceso Interno

```
START ETL PIPELINE
    â”‚
    â”œâ”€ [1/5] populate_dim_customer()
    â”‚        â””â”€ SELECT DISTINCT customer + SUM(sales)
    â”‚        â””â”€ INSERT 5,234 clientes Ãºnicos
    â”‚        â””â”€ Output: {customer_id â†’ customer_key} lookup
    â”‚
    â”œâ”€ [2/5] populate_dim_geography()
    â”‚        â””â”€ SELECT DISTINCT (market, region, country, state, city)
    â”‚        â””â”€ Validar mercados âˆˆ {Africa, Europe, LATAM, Pacific Asia, USCA}
    â”‚        â””â”€ INSERT 985 combinaciones geogrÃ¡ficas
    â”‚        â””â”€ Output: {(market,region,...) â†’ geography_id} lookup
    â”‚
    â”œâ”€ [3/5] populate_dim_product()
    â”‚        â””â”€ SELECT DISTINCT product + category + department
    â”‚        â””â”€ INSERT 1,812 productos Ãºnicos
    â”‚        â””â”€ Output: {product_card_id â†’ product_key} lookup
    â”‚
    â”œâ”€ [4/5] populate_dim_date()
    â”‚        â””â”€ MIN/MAX order_date desde staging
    â”‚        â””â”€ Generar calendario: 2020-01-01 to 2024-12-31 (1,826 dÃ­as)
    â”‚        â””â”€ Calcular: year, quarter, month, week, day_of_week, is_weekend
    â”‚        â””â”€ Output: {order_date â†’ date_id (YYYYMMDD)} lookup
    â”‚
    â”œâ”€ [5/5] populate_fact_orders()
    â”‚        â”œâ”€ READ staging (186,523 unprocessed order items)
    â”‚        â”œâ”€ MAP foreign keys usando lookups
    â”‚        â”œâ”€ VALIDATE: No NULLs en FKs
    â”‚        â”œâ”€ CALCULATE:
    â”‚        â”‚   â””â”€ is_otif = (late_delivery_risk = 0)
    â”‚        â”‚   â””â”€ revenue_at_risk = sales * late_delivery_risk
    â”‚        â”‚   â””â”€ etl_run_id = UUID (auditorÃ­a)
    â”‚        â”œâ”€ DETECT ANOMALIES: days > 60, discount > 100%
    â”‚        â”œâ”€ INSERT fact_orders (batch, 1000/batch)
    â”‚        â””â”€ Output: 186,289 filas + 3 KPIs
    â”‚
    â””â”€ COMMIT & MARK
             â””â”€ UPDATE stg_raw_orders SET is_processed = TRUE
             â””â”€ Log: Elapsed time, metrics, ETL run ID
END ETL PIPELINE
```

### Datos Intermedios (Lookups)

Durante la ejecuciÃ³n se crean 4 diccionarios en memoria:

```python
customer_lookup = {
    'customer_1001': 1,
    'customer_1002': 2,
    ...  # 5,234 entries
}

geography_lookup = {
    ('Africa', 'North Africa', 'Egypt', 'Cairo', 'Cairo'): 1,
    ('Europe', 'Western Europe', 'Spain', 'Madrid', 'Madrid'): 2,
    ...  # 985 entries
}

product_lookup = {
    'PROD-001': 1,
    'PROD-002': 2,
    ...  # 1,812 entries
}

date_lookup = {
    Timestamp('2020-01-01'): 20200101,
    Timestamp('2020-01-02'): 20200102,
    ...  # 1,826 entries
}
```

Estos lookups se usan para mapear FKs en fact_orders.

---

## ðŸ”„ CÃ³mo Ejecutar

### OpciÃ³n 1: Pipeline Completo (RECOMENDADO)

```bash
# En una terminal, desde raÃ­z del proyecto
make run
```

**Esto ejecuta:**
1. `make install` - Instalar dependencias
2. `make setup-docker` - Iniciar PostgreSQL
3. `make load-raw` - Cargar CSVs â†’ stg_raw_orders
4. `make validate-transform` - Pre-flight checks â­ NUEVO
5. `make transform` - Ejecutar ETL â­ NUEVO
6. `make export` - Exportar CSVs para Power BI
7. `make validate` - ValidaciÃ³n de calidad

**Tiempo total:** ~10-15 minutos

### OpciÃ³n 2: Solo TransformaciÃ³n (Desarrollo)

```bash
# Si ya cargaste datos con make load-raw

# Pre-flight checks (optional pero recomendado)
python scripts/validate_transform.py

# Ejecutar transformaciÃ³n
python scripts/transform_data.py
```

**Tiempo:** ~3-5 minutos (sin Docker startup ni CSV load)

### OpciÃ³n 3: Step-by-Step (Debugging)

```bash
# Terminal 1: PostgreSQL
docker-compose -f config/docker-compose.yml up -d

# Terminal 2: Cargar datos
python scripts/load_data.py

# Terminal 3: Validar
python scripts/validate_transform.py

# Terminal 4: Transformar
python scripts/transform_data.py

# Terminal 5: Exportar
python src/etl/export_star_schema.py
```

---

## ðŸ“Š QuÃ© Esperar en Logs

### Pre-flight Validation
```
ðŸ” Validating database connection...
  âœ… PostgreSQL connection OK

ðŸ” Validating schema structure...
  âœ… Schema 'dw' exists

ðŸ” Validating required tables...
  âœ… stg_raw_orders: Staging table (input)
  âœ… dim_customer: Customer dimension
  âœ… dim_product: Product dimension
  âœ… dim_geography: Geography dimension
  âœ… dim_date: Date dimension
  âœ… fact_orders: Orders fact table

ðŸ” Validating staging data...
  âœ… stg_raw_orders: 186,523 rows
  âœ… Unprocessed rows: 186,523

ðŸ” Validating critical fields...
  âœ… customer_id: Customer identification
  âœ… order_id: Order identification
  âœ… order_date: Order date
  âœ… market: Market (geography)
  âœ… sales: Sales amount
  âœ… late_delivery_risk: Delivery risk flag

âœ… VALIDATION SUCCESSFUL - Ready to run transform_data.py
```

### TransformaciÃ³n
```
================================================================================
TORRE CONTROL - ETL PIPELINE: Stage â†’ Star Schema
Start Time: 2026-02-04 14:30:15
ETL Run ID: a1b2c3d4-e5f6-7890-abcd-ef1234567890
================================================================================

ðŸ”„ [1/5] Populating dim_customer...
  ðŸ“¥ Read 5,234 unique customers from staging
  Inserting customers: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 34%
âœ… dim_customer: 5,234 inserted/updated

ðŸ”„ [2/5] Populating dim_geography...
  ðŸ“¥ Read 987 unique geographic combinations
  âš ï¸  Invalid markets detected: ['Unknown']. Filtering out.
  âœ… Validated 985 geographic records
  Inserting geographies: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 56%
âœ… dim_geography: 985 inserted

ðŸ”„ [3/5] Populating dim_product...
  ðŸ“¥ Read 1,812 unique products from staging
  Inserting products: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 67%
âœ… dim_product: 1,812 inserted

ðŸ”„ [4/5] Populating dim_date...
  ðŸ“… Date range: 2020-01-01 to 2024-12-31
  ðŸ“… Generated 1,826 calendar dates
  Inserting dates: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 89%
âœ… dim_date: 1,826 inserted

ðŸ”„ [5/5] Populating fact_orders...
  ðŸ“¥ Read 186,523 unprocessed order items from staging
  âš ï¸  customer_key: 145 NULLs (rows will be skipped)
  âš ï¸  geography_key: 89 NULLs (rows will be skipped)
  âš ï¸  product_key: 0 NULLs (rows will be skipped)
  âš ï¸  date_key: 0 NULLs (rows will be skipped)
  âœ… Valid fact rows: 186,289 (skipped: 234)
  âš ï¸  Detected 34 anomalies (delay>60d or discount>100%)
  Inserting facts: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 78%
  ðŸ“ˆ OTIF%: 84.23%
  ðŸ’° Revenue at Risk: $1,234,567.89
âœ… fact_orders: 186,289 inserted/updated

ðŸ”„ Marking staging as processed...
âœ… Staging marked as processed

================================================================================
âœ… ETL PIPELINE SUCCESSFUL
Elapsed Time: 187.4 seconds
Fact Summary: {'total_orders': 186523, 'inserted': 186289, 'skipped': 234, 
               'otif_pct': 84.23, 'revenue_at_risk': 1234567.89}
End Time: 2026-02-04 14:33:22
================================================================================
```

---

## ðŸ“ Archivos Generados

DespuÃ©s de ejecutar `make run`, tendrÃ¡s:

```
Data/Processed/
â”œâ”€ fact_orders.csv           (186,289 rows, ~50MB)
â”œâ”€ dim_customer.csv          (5,234 rows, ~200KB)
â”œâ”€ dim_product.csv           (1,812 rows, ~150KB)
â”œâ”€ dim_geography.csv         (985 rows, ~50KB)
â””â”€ dim_date.csv              (1,826 rows, ~100KB)

logs/
â”œâ”€ transform_data.log        â† Log del ETL (nuevo)
â”œâ”€ validate_transform.log    â† Log de validaciÃ³n (nuevo)
â””â”€ load_data_output.txt      â† Log de carga

PostgreSQL (supply_chain_dw):
â”œâ”€ dw.stg_raw_orders         (is_processed = TRUE)
â”œâ”€ dw.dim_customer           (5,234 rows)
â”œâ”€ dw.dim_product            (1,812 rows)
â”œâ”€ dw.dim_geography          (985 rows)
â”œâ”€ dw.dim_date               (1,826 rows)
â””â”€ dw.fact_orders            (186,289 rows, 18 columnas + indices)
```

---

## ðŸŽ¯ KPIs Disponibles Post-Transform

| KPI | CÃ¡lculo | UbicaciÃ³n | Valor Ejemplo |
|-----|---------|-----------|---------------|
| **OTIF%** | (on_time âˆ§ in_full) / total | fact_orders.is_otif | 84.23% |
| **Revenue at Risk** | SUM(sales) WHERE late=1 | fact_orders.revenue_at_risk | $1.23M |
| **Late Delivery Rate** | COUNT(late) / total | Derived | 15.77% |
| **Avg Delay Days** | AVG(days_real - days_scheduled) | Derived | 3.2 days |
| **Anomaly Rate** | COUNT(anomalies) / total | Detected | 0.018% |

---

## ðŸ”— IntegraciÃ³n con Power BI

DespuÃ©s de `make run`, tendrÃ¡s CSVs listos para importar en Power BI:

```
PBIX/TorreControl_v0.1.pbix
â”œâ”€ Data Model
â”‚  â”œâ”€ fact_orders (import Data/Processed/fact_orders.csv)
â”‚  â”œâ”€ dim_customer (import Data/Processed/dim_customer.csv)
â”‚  â”œâ”€ dim_product (import Data/Processed/dim_product.csv)
â”‚  â”œâ”€ dim_geography (import Data/Processed/dim_geography.csv)
â”‚  â””â”€ dim_date (import Data/Processed/dim_date.csv)
â”‚
â””â”€ Dashboard Views
   â”œâ”€ Q1: OTIF Performance (Market Ã— Segment drill-down)
   â”œâ”€ Q2: Revenue at Risk (Waterfall + Top drivers)
   â”œâ”€ Q3: VIP Churn Risk (Table + Trend)
   â”œâ”€ Q4: Geographic Efficiency (Map drill-down)
   â””â”€ Q5: Anomaly Detection (Fraud + Outliers)
```

---

## âš™ï¸ ConfiguraciÃ³n Recomendada

### `.env` (Opcional, para override)
```bash
DATABASE_URL=postgresql://admin:adminpassword@localhost:5433/supply_chain_dw
LOG_DIR=./logs
```

### `docker-compose.yml` (Ya configurado)
```yaml
services:
  postgres:
    image: postgres:15
    ports:
      - "5433:5432"  â† Puerto no-estÃ¡ndar para no chocar con otros services
    environment:
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: adminpassword
      POSTGRES_DB: supply_chain_dw
```

### `requirements.txt` (Dependencias)
```
SQLAlchemy>=2.0.0
pandas>=1.5.0
python-dotenv>=0.21.0
tqdm>=4.64.0
psycopg2-binary>=2.9.0
```

---

## ðŸ†˜ Troubleshooting Common Issues

### Problema: "Port 5433 already in use"
```bash
# Encontrar quÃ© usa ese puerto
lsof -i :5433  # Mac/Linux
netstat -ano | findstr :5433  # Windows PowerShell

# O usar otro puerto en docker-compose.yml
# Cambiar "5433:5432" a "5434:5432"
```

### Problema: "schema.dw does not exist"
```bash
# Regenerar DDL
psql -U admin -d supply_chain_dw -f sql/ddl/01_schema_base.sql
```

### Problema: "is_processed column not found"
```bash
# Actualizar schema (agregar columna faltante)
# En PostgreSQL:
ALTER TABLE dw.stg_raw_orders 
ADD COLUMN is_processed BOOLEAN DEFAULT FALSE;

CREATE INDEX idx_stg_raw_unprocessed 
ON dw.stg_raw_orders(is_processed) 
WHERE is_processed = FALSE;
```

### Problema: "Memory error on large batch"
```python
# En transform_data.py, reducir batch size:
batch_size = 500  # Default 1000, reducir a 500 o 250
```

---

## ðŸ“ˆ Benchmarks

| Fase | Script | Filas | Tiempo | Rows/Sec |
|------|--------|-------|--------|----------|
| Load | load_data.py | 100K â†’ 186K items | 10-20s | ~9K-18K |
| Transform | transform_data.py | 186K items â†’ 5 tables | 180-200s | ~930 |
| Export | export_star_schema.py | 5 tables â†’ CSVs | 5-10s | N/A |
| Validate | load_data.py --validate | 5 tables | 2-3s | N/A |
| **TOTAL** | **make run** | **Complete pipeline** | **~10-15 min** | **N/A** |

---

## ðŸŽ“ Learning Resources

| Tema | Archivo | DescripciÃ³n |
|------|---------|-------------|
| GuÃ­a TÃ©cnica Completa | [TRANSFORM_DATA_GUIDE.md](docs/guides/TRANSFORM_DATA_GUIDE.md) | 600+ lÃ­neas con detalles internos |
| Quick Start | [TRANSFORM_DATA_QUICK_START.md](docs/guides/TRANSFORM_DATA_QUICK_START.md) | Cheat sheet de ejecuciÃ³n |
| CÃ³digo Fuente | [scripts/transform_data.py](scripts/transform_data.py) | 600+ lÃ­neas comentado |
| ValidaciÃ³n | [scripts/validate_transform.py](scripts/validate_transform.py) | Pre-flight checks |
| DocumentaciÃ³n ETL | [CONTEXTO_ESTRATEGICO.md](docs/guides/CONTEXTO_ESTRATEGICO.md) | Context de los 5 KPIs |

---

## âœ… Checklist EjecuciÃ³n

Antes de ejecutar `make run`:

- [ ] PostgreSQL instalado y corriendo
- [ ] Docker instalado (`docker --version`)
- [ ] Python 3.10+ instalado
- [ ] Git clonado el repositorio
- [ ] `.venv` creado (`python -m venv .venv`)
- [ ] Dependencias instaladas (`pip install -r requirements.txt`)

En `make run`:

- [ ] Fase 1: Load Raw completa sin errores
- [ ] Fase 2: ValidaciÃ³n pre-transform exitosa
- [ ] Fase 2: Transform completa en 180-200 segundos
- [ ] Fase 3: Export genera 5 CSVs
- [ ] Fase 4: Validate muestra OTIF% > 80%
- [ ] Logs muestran KPIs calculados

Post-Pipeline:

- [ ] CSVs en `Data/Processed/` son accesibles
- [ ] Power BI importa sin errores
- [ ] Dashboard muestra datos (no vacÃ­o)
- [ ] Drill-downs funcionan (Market â†’ Region â†’ State â†’ City)

---

**VersiÃ³n:** 1.0  
**Ãšltima ActualizaciÃ³n:** 4 Feb 2026  
**Estado:** âœ… Production Ready
