# Transform Data Pipeline - Technical Guide
## `scripts/transform_data.py`

**Versi√≥n:** 1.0  
**Fecha:** 4 de febrero de 2026  
**Estado:** ‚úÖ Production Ready  

---

## üìã Tabla de Contenidos

1. [Descripci√≥n General](#descripci√≥n-general)
2. [Arquitectura](#arquitectura)
3. [Funciones Principales](#funciones-principales)
4. [Flujo ETL](#flujo-etl)
5. [Validaciones Cr√≠ticas](#validaciones-cr√≠ticas)
6. [C√≥mo Ejecutar](#c√≥mo-ejecutar)
7. [Monitoreo y Logs](#monitoreo-y-logs)
8. [Troubleshooting](#troubleshooting)

---

## üéØ Descripci√≥n General

`transform_data.py` es el **orquestador central del ETL** que convierte datos crudos del staging en un **Star Schema anal√≠tico** listo para Power BI.

### Flujo de Datos

```
CSV Raw (DataCoSupplyChainDataset.csv)
            ‚Üì
    PostgreSQL Staging (dw.stg_raw_orders)
            ‚Üì
    TRANSFORM PIPELINE (scripts/transform_data.py)
            ‚îú‚îÄ Populate Dimensions (customer, product, geography, date)
            ‚îî‚îÄ Populate Facts (fact_orders con m√©tricas calculadas)
            ‚Üì
    Power BI-Ready Star Schema
            ‚îú‚îÄ dim_customer.csv
            ‚îú‚îÄ dim_product.csv
            ‚îú‚îÄ dim_geography.csv
            ‚îú‚îÄ dim_date.csv
            ‚îî‚îÄ fact_orders.csv
```

### Caracter√≠sticas Clave

‚úÖ **Batch Processing** - Procesa miles de filas eficientemente con `tqdm`  
‚úÖ **Transactional Integrity** - Usa transacciones expl√≠citas para rollback en caso de error  
‚úÖ **Audit Trail** - `etl_run_id` (UUID) para rastrear qu√© corre en cada ejecuci√≥n  
‚úÖ **Lookups Eficientes** - Diccionarios en memoria para mapeos FK  
‚úÖ **Validaci√≥n de Datos** - NULLs, outliers, anomal√≠as  
‚úÖ **Logging Detallado** - Timestamps, contadores, m√©tricas KPI  

---

## üèóÔ∏è Arquitectura

### Componentes

| Componente | Prop√≥sito | Salida |
|------------|-----------|--------|
| `populate_dim_customer()` | Extraer clientes √∫nicos, calcular LTV | `{customer_id: customer_key}` lookup |
| `populate_dim_geography()` | Crear jerarqu√≠a geogr√°fica (Market‚ÜíRegion‚ÜíState‚ÜíCity) | `{(market, region, ...): geography_id}` lookup |
| `populate_dim_product()` | Extraer productos √∫nicos | `{product_card_id: product_key}` lookup |
| `populate_dim_date()` | Generar calendario completo con atributos temporales | `{order_date: date_id}` lookup |
| `populate_fact_orders()` | JOIN staging con dims, calcular m√©tricas, insertar hechos | Row count + KPIs |
| `run_etl_pipeline()` | Orquestar secuencia, manejo de errores, auditor√≠a | Exit code (0=√©xito) |

### Dependencias

```python
# SQLAlchemy - Database ORM
from sqlalchemy import create_engine, text
from sqlalchemy.exc import IntegrityError, SQLAlchemyError

# pandas - Data transformation
import pandas as pd

# tqdm - Progress bars
from tqdm import tqdm

# Standard library
import logging, uuid, os, pathlib, datetime
```

### Base de Datos

```sql
-- Connection
postgresql://admin:adminpassword@localhost:5433/supply_chain_dw

-- Staging Table (Input)
dw.stg_raw_orders (54 campos)

-- Dimension Tables (Output)
dw.dim_customer
dw.dim_product
dw.dim_geography
dw.dim_date

-- Fact Table (Output)
dw.fact_orders
```

---

## üîß Funciones Principales

### 1. `populate_dim_customer(engine)`

**Prop√≥sito:** Extraer clientes √∫nicos del staging y crear tabla de dimensi√≥n.

**L√≥gica:**
1. SELECT DISTINCT customer_id, fname, lname, email, segment desde `stg_raw_orders`
2. GROUP BY customer_id y calcular `sales_per_customer = SUM(sales)`
3. Concatenar nombre completo: `customer_name = fname || ' ' || lname`
4. INSERT con UPSERT (ON CONFLICT DO UPDATE)

**Validaciones:**
- ‚ö†Ô∏è Si `customer_id` o `customer_name` es NULL ‚Üí Skip
- ‚úÖ Log: "{count} NULLs detected (will skip)"

**Retorna:**
```python
{
    'customer_1001': 1,      # customer_id ‚Üí customer_key
    'customer_1002': 2,
    ...
}
```

**Ejemplo de Inserci√≥n:**
```sql
INSERT INTO dw.dim_customer 
(customer_id, customer_name, customer_email, customer_segment, sales_per_customer)
VALUES ('customer_1001', 'John Doe', 'john@example.com', 'Consumer', 5234.56)
ON CONFLICT (customer_id) 
DO UPDATE SET 
    customer_name = EXCLUDED.customer_name,
    sales_per_customer = EXCLUDED.sales_per_customer
RETURNING customer_key;
```

**Output del Log:**
```
üîÑ [1/5] Populating dim_customer...
  üì• Read 5,234 unique customers from staging
  üìå Customer lookup dict: 5,234 entries
‚úÖ dim_customer: 5,234 inserted/updated
```

---

### 2. `populate_dim_geography(engine)`

**Prop√≥sito:** Crear jerarqu√≠a geogr√°fica con validaci√≥n de mercados v√°lidos.

**L√≥gica:**
1. SELECT DISTINCT market, order_region, customer_country, customer_state, customer_city
2. Validar `market ‚àà {Africa, Europe, LATAM, Pacific Asia, USCA}`
3. Llenar NULLs en campos jer√°rquicos con "Unknown"
4. INSERT con clave compuesta √∫nica

**Validaciones:**
- ‚úÖ Market blanco: Skip + warning
- ‚úÖ Mercados inv√°lidos: Filter + warning
- ‚úÖ Region/Country/State/City NULL ‚Üí Fill con "Unknown"

**Retorna:**
```python
{
    ('Africa', 'North Africa', 'Egypt', 'Cairo', 'Cairo'): 1,
    ('Europe', 'Western Europe', 'Spain', 'Madrid', 'Madrid'): 2,
    ...
}
```

**Unique Constraint:**
```sql
CREATE UNIQUE INDEX idx_geo_unique 
ON dw.dim_geography (market, region, country, state, city);
```

**Output del Log:**
```
üîÑ [2/5] Populating dim_geography...
  üì• Read 987 unique geographic combinations
  ‚ö†Ô∏è  Invalid markets detected: ['Unknown', 'NULL']. Filtering out.
  ‚úÖ Validated 985 geographic records
üìå Geography lookup dict: 985 entries
‚úÖ dim_geography: 985 inserted
```

---

### 3. `populate_dim_product(engine)`

**Prop√≥sito:** Extraer productos √∫nicos y mapear a tabla dimensional.

**L√≥gica:**
1. SELECT DISTINCT product_card_id, product_name, category_name, department_name, product_price
2. Llenar NULLs: `product_name = 'Unknown'`, `category_name = 'Unknown'`
3. Validar `product_price` (numeric, default 0.0)
4. INSERT con ON CONFLICT DO NOTHING (idempotente)

**Retorna:**
```python
{
    'PROD-001': 1,      # product_card_id ‚Üí product_key
    'PROD-002': 2,
    ...
}
```

**Output del Log:**
```
üîÑ [3/5] Populating dim_product...
  üì• Read 1,812 unique products from staging
‚úÖ dim_product: 1,812 inserted
  üìå Product lookup dict: 1,812 entries
```

---

### 4. `populate_dim_date(engine)`

**Prop√≥sito:** Generar dimensi√≥n de fecha completa con atributos temporales.

**L√≥gica:**
1. Extraer rango: MIN(order_date) y MAX(order_date) desde staging
2. Generar calendario completo con `pd.date_range()`
3. Calcular:
   - `date_id = YYYYMMDD` (ej: 20230115)
   - `year, quarter, month, week, day_of_month`
   - `day_of_week (1=Monday), month_name, day_name, is_weekend`
4. INSERT (ON CONFLICT DO NOTHING)

**Ejemplo de Datos Generados:**
```
date_id  | order_date | year | quarter | month | day_of_week | month_name | is_weekend
---------|------------|------|---------|-------|-------------|------------|------------
20230101 | 2023-01-01 | 2023 |    1    |   1   |      7      | January    |     1
20230102 | 2023-01-02 | 2023 |    1    |   1   |      1      | January    |     0
...
```

**Retorna:**
```python
{
    Timestamp('2023-01-01'): 20230101,
    Timestamp('2023-01-02'): 20230102,
    ...
}
```

**Output del Log:**
```
üîÑ [4/5] Populating dim_date...
  üìÖ Date range: 2020-01-01 to 2024-12-31
  üìÖ Generated 1,826 calendar dates
‚úÖ dim_date: 1,826 inserted
  üìå Date lookup dict: 1,826 entries
```

---

### 5. `populate_fact_orders(engine, customer_lookup, ...)`

**Prop√≥sito:** Poblaci√≥n de la tabla de hechos con JOINs a todas las dimensiones.

**L√≥gica:**
1. READ staging con WHERE `is_processed = FALSE`
2. MAP foreign keys usando lookups:
   - `customer_id` ‚Üí `dim_customer.customer_key`
   - `(market, region, ...) ‚Üí dim_geography.geography_id`
   - `product_card_id` ‚Üí `dim_product.product_key`
   - `order_date` ‚Üí `dim_date.date_id`
3. VALIDAR: No permitir NULLs en FKs
4. CALCULAR m√©tricas:
   - `is_otif = (late_delivery_risk = 0) ? 1 : 0`
   - `revenue_at_risk = sales * late_delivery_risk`
5. DETECTAR anomal√≠as:
   - `days_for_shipping_real > 60` ‚Üí Flag
   - `order_item_discount_rate > 100%` ‚Üí Impossible value
6. INSERT batch con `executemany()` (r√°pido)

**Batch Insert Optimization:**
```python
# Procesa 1,000 filas por batch para optimizar I/O
for batch_start in range(0, len(df_facts), batch_size=1000):
    # Prepare values list
    values_list = [...]
    # Execute all at once
    conn.execute(upsert_query, values_list)
```

**Validaciones Cr√≠ticas:**
| FK | Nulls Permitidos | Acci√≥n |
|----|------------------|--------|
| customer_key | ‚ùå No | Skip row + Warning |
| geography_key | ‚ùå No | Skip row + Warning |
| product_key | ‚ùå No | Skip row + Warning |
| date_key | ‚ùå No | Skip row + Warning |

**Output del Log:**
```
üîÑ [5/5] Populating fact_orders...
  üì• Read 186,523 unprocessed order items from staging
  ‚ö†Ô∏è  customer_key: 145 NULLs (rows will be skipped)
  ‚ö†Ô∏è  geography_key: 89 NULLs (rows will be skipped)
  ‚úÖ Valid fact rows: 186,289 (skipped: 234)
  ‚ö†Ô∏è  Detected 34 anomalies (delay>60d or discount>100%)
  üìà OTIF%: 84.23%
  üí∞ Revenue at Risk: $1,234,567.89
‚úÖ fact_orders: 186,289 inserted/updated
```

---

### 6. `run_etl_pipeline()`

**Prop√≥sito:** Orquestar la secuencia ETL completa con manejo de transacciones y auditor√≠a.

**Algoritmo:**
```
START ETL
‚îú‚îÄ Generate etl_run_id (UUID)
‚îú‚îÄ Connect to PostgreSQL
‚îú‚îÄ [1] populate_dim_customer()
‚îú‚îÄ [2] populate_dim_geography()
‚îú‚îÄ [3] populate_dim_product()
‚îú‚îÄ [4] populate_dim_date()
‚îú‚îÄ [5] populate_fact_orders(etl_run_id)
‚îú‚îÄ COMMIT: UPDATE stg_raw_orders SET is_processed = TRUE
‚îú‚îÄ Log: Elapsed time, metrics
‚îî‚îÄ END ETL (exit code 0)

IF ERROR:
‚îú‚îÄ ROLLBACK all changes
‚îú‚îÄ Log: Full error stack + context
‚îî‚îÄ EXIT (code 1)
```

**Transactional Safety:**
```python
try:
    engine.begin() as conn:  # Auto-commit on success, rollback on exception
        # All transforms happen here
        populate_dim_customer(engine)
        populate_fact_orders(engine, ...)
        # If ANY fails, all rollback
except Exception as e:
    logger.error(f"ETL Failed: {e}")
    return 1  # Exit with error code
finally:
    engine.dispose()  # Clean up connections
```

**Output del Log Completo:**
```
================================================================================
TORRE CONTROL - ETL PIPELINE: Stage ‚Üí Star Schema
Start Time: 2026-02-04 14:30:15
ETL Run ID: a1b2c3d4-e5f6-7890-abcd-ef1234567890
================================================================================

üîÑ [1/5] Populating dim_customer...
  ‚úÖ dim_customer: 5,234 inserted/updated

üîÑ [2/5] Populating dim_geography...
  ‚úÖ dim_geography: 985 inserted

üîÑ [3/5] Populating dim_product...
  ‚úÖ dim_product: 1,812 inserted

üîÑ [4/5] Populating dim_date...
  ‚úÖ dim_date: 1,826 inserted

üîÑ [5/5] Populating fact_orders...
  üìà OTIF%: 84.23%
  üí∞ Revenue at Risk: $1,234,567.89
  ‚úÖ fact_orders: 186,289 inserted/updated

üîÑ Marking staging as processed...
‚úÖ Staging marked as processed

================================================================================
‚úÖ ETL PIPELINE SUCCESSFUL
Elapsed Time: 187.4 seconds
Fact Summary: {'total_orders': 186523, 'inserted': 186289, 'skipped': 234, 'otif_pct': 84.23, 'revenue_at_risk': 1234567.89}
End Time: 2026-02-04 14:33:22
================================================================================
```

---

## üîÑ Flujo ETL Detallado

### Fase 1: Preparaci√≥n
```
‚úì Load environment variables (.env)
‚úì Create logs directory
‚úì Setup logging handlers (file + console)
‚úì Connect to PostgreSQL (test connection)
```

### Fase 2: Transformaci√≥n de Dimensiones
```
‚îå‚îÄ dim_customer
‚îÇ  ‚îú‚îÄ SELECT DISTINCT + GROUP BY
‚îÇ  ‚îú‚îÄ Calculate sales_per_customer
‚îÇ  ‚îî‚îÄ INSERT (UPSERT)
‚îÇ
‚îú‚îÄ dim_geography
‚îÇ  ‚îú‚îÄ SELECT DISTINCT combinations
‚îÇ  ‚îú‚îÄ Validate markets
‚îÇ  ‚îî‚îÄ INSERT (compound unique key)
‚îÇ
‚îú‚îÄ dim_product
‚îÇ  ‚îú‚îÄ SELECT DISTINCT products
‚îÇ  ‚îú‚îÄ Fill NULLs
‚îÇ  ‚îî‚îÄ INSERT (DO NOTHING)
‚îÇ
‚îî‚îÄ dim_date
   ‚îú‚îÄ Extract date range
   ‚îú‚îÄ Generate calendar
   ‚îú‚îÄ Calculate temporal attributes
   ‚îî‚îÄ INSERT (DO NOTHING)
```

### Fase 3: Transformaci√≥n de Hechos
```
‚îå‚îÄ Read staging
‚îú‚îÄ Map foreign keys (customer, geography, product, date)
‚îú‚îÄ Validate FK referential integrity
‚îú‚îÄ Calculate OTIF, Revenue at Risk
‚îú‚îÄ Detect anomalies
‚îú‚îÄ INSERT facts (batch, 1000 rows/batch)
‚îî‚îÄ Mark staging as processed
```

### Fase 4: Post-Processing
```
‚úì Calculate OTIF%, Revenue at Risk, Churn Risk
‚úì Log metrics to file
‚úì Close database connections
‚úì Return exit code (0=success, 1=failure)
```

---

## üõ°Ô∏è Validaciones Cr√≠ticas

### 1. Integridad Referencial

**Validaci√≥n:** Asegurar que todos los FKs existan en dimensiones.

```python
# Si customer_id no existe en dim_customer ‚Üí skip row
if row['customer_key'] is None:
    logger.warning(f"customer_id {customer_id} not in dim_customer, skipping")
    # Row no se inserta en fact_orders
```

**Output:**
```
‚ö†Ô∏è  customer_key: 145 NULLs (rows will be skipped)
‚úÖ Valid fact rows: 186,289 (skipped: 234)
```

### 2. Detecci√≥n de Outliers

**An√≥malos Detectados:**
- `days_for_shipping_real > 60` ‚Üí Posible p√©rdida o data error
- `order_item_discount_rate > 100%` ‚Üí Imposible matem√°ticamente

```python
anomalies = df_facts_valid[
    (df_facts_valid["days_for_shipping_real"] > 60) |
    (df_facts_valid["order_item_discount_rate"] > 100)
]
if len(anomalies) > 0:
    logger.warning(f"Detected {len(anomalies)} anomalies")
```

**Output:**
```
‚ö†Ô∏è  Detected 34 anomalies (delay>60d or discount>100%)
```

### 3. Validaci√≥n de Mercados

**Mercados V√°lidos:** `{Africa, Europe, LATAM, Pacific Asia, USCA}`

```python
valid_markets = {"Africa", "Europe", "LATAM", "Pacific Asia", "USCA"}
invalid = df_geo[~df_geo["market"].isin(valid_markets)]["market"].unique()
if len(invalid) > 0:
    logger.warning(f"Invalid markets: {invalid}. Filtering out.")
```

### 4. Null Checks en Campos Cr√≠ticos

| Campo | Tabla | Acci√≥n |
|-------|-------|--------|
| customer_id | dim_customer, fact_orders | Skip row si NULL |
| order_id | fact_orders | Skip row si NULL |
| order_date | dim_date | Skip row si NULL |
| market | dim_geography | Skip row si NULL |

---

## üöÄ C√≥mo Ejecutar

### Opci√≥n 1: Via Makefile (Recomendado)

```bash
# Ejecutar todo el pipeline
make run

# Solo ejecutar la fase de transformaci√≥n
make transform
```

### Opci√≥n 2: Directo desde Python

```bash
# Activar venv
.venv\Scripts\Activate.ps1  # Windows PowerShell
source .venv/bin/activate   # Mac/Linux

# Ejecutar script
python scripts/transform_data.py
```

### Opci√≥n 3: Con Variables de Entorno Personalizadas

```bash
# .env file
export DATABASE_URL="postgresql://user:pass@host:5433/db"
export LOG_DIR="./custom_logs"

# Ejecutar
python scripts/transform_data.py
```

### Requisitos Previos

‚úÖ PostgreSQL corriendo en puerto 5433  
‚úÖ Base de datos `supply_chain_dw` creada  
‚úÖ Schema `dw` con tablas staging + dimensiones + hechos  
‚úÖ Datos cargados en `dw.stg_raw_orders`  
‚úÖ Python 3.10+ con dependencias instaladas  

```bash
pip install -r requirements.txt
# Requiere: sqlalchemy, pandas, python-dotenv, tqdm
```

---

## üìä Monitoreo y Logs

### Archivos de Log

```
logs/transform_data.log  ‚Üê Log principal (DEBUG + INFO + ERROR)
logs/load_data_output.txt  ‚Üê Log del ETL anterior
```

### C√≥mo Leer los Logs

```bash
# Ver √∫ltimas 100 l√≠neas
tail -100 logs/transform_data.log

# Filtrar solo errores
grep "‚ùå" logs/transform_data.log

# Ver KPIs finales
grep "OTIF%\|Revenue at Risk" logs/transform_data.log
```

### Ejemplo de Log Exitoso

```
================================================================================
‚úÖ ETL PIPELINE SUCCESSFUL
Elapsed Time: 187.4 seconds
Fact Summary: {'total_orders': 186523, 'inserted': 186289, 'skipped': 234, 
               'otif_pct': 84.23, 'revenue_at_risk': 1234567.89}
================================================================================
```

### Ejemplo de Log con Errores

```
================================================================================
‚ùå Database error: (psycopg2.OperationalError) could not connect to server
ERR: Database connection failed
   Make sure PostgreSQL is running: 'docker-compose -f config/docker-compose.yml up -d'
================================================================================
```

---

## üîß Troubleshooting

### Problema: "Connection refused on localhost:5433"

**Causa:** PostgreSQL no est√° corriendo

**Soluci√≥n:**
```bash
docker-compose -f config/docker-compose.yml up -d
# Esperar 5 segundos para que PostgreSQL se inicie
sleep 5
python scripts/transform_data.py
```

### Problema: "Foreign key constraint violation"

**Causa:** Dimensi√≥n faltante para una fila de hecho

**Soluci√≥n:** Script lo maneja autom√°ticamente:
```
‚ö†Ô∏è  customer_key: 145 NULLs (rows will be skipped)
‚úÖ Valid fact rows: 186,289 (skipped: 234)
```

Filas con FKs faltantes son **skipped**, no causan error.

### Problema: "is_processed column not found"

**Causa:** Schema `dw.stg_raw_orders` no est√° actualizado

**Soluci√≥n:**
```bash
# Regenerar schema desde DDL
psql -U admin -d supply_chain_dw -f sql/ddl/01_schema_base.sql
```

### Problema: "Memory error on large batch"

**Causa:** Batch size demasiado grande

**Soluci√≥n:** Reducir tama√±o de batch en c√≥digo:
```python
batch_size = 500  # Default 1000, reducir a 500
```

### Problema: "Encoding error: latin1"

**Causa:** Datos con caracteres especiales

**Soluci√≥n:** Script usa UTF-8, pero CSV raw es ISO-8859-1. Asegurar que `load_data.py` convierte:
```python
df = pd.read_csv("file.csv", encoding="ISO-8859-1")
```

---

## üìà KPIs Calculados

| KPI | F√≥rmula | Ubicaci√≥n |
|-----|---------|-----------|
| **OTIF%** | (on_time ‚àß in_full) / total_orders | fact_orders.is_otif |
| **Revenue at Risk** | SUM(sales) WHERE late_delivery_risk=1 | fact_orders.revenue_at_risk |
| **Late Delivery Rate** | COUNT(*) WHERE late_delivery_risk=1 / total | Calculado en queries |
| **Churn Risk Score** | TOP 10% by sales + ‚â•2 late orders | Analytics view |

### C√°lculo de OTIF

```python
# En populate_fact_orders()
df_facts_valid["is_otif"] = (
    (df_facts_valid["late_delivery_risk"] == 0).astype(int)
)

# En logs finales
otif_pct = (df_facts_valid["is_otif"].sum() / len(df_facts_valid)) * 100
logger.info(f"üìà OTIF%: {otif_pct:.2f}%")
```

---

## üìö Referencias

- [PostgreSQL 15 Documentation](https://www.postgresql.org/docs/15/)
- [SQLAlchemy ORM Tutorial](https://docs.sqlalchemy.org/en/20/)
- [pandas DataFrame API](https://pandas.pydata.org/docs/)
- [Torre Control Project Structure](../CONTEXTO_ESTRATEGICO.md)

---

## ‚úÖ Checklist Pre-Producci√≥n

- [ ] PostgreSQL corriendo en puerto 5433
- [ ] `dw.stg_raw_orders` con datos
- [ ] DDL schema completo
- [ ] `.env` con `DATABASE_URL` correcto
- [ ] `requirements.txt` instalado
- [ ] Primera ejecuci√≥n exitosa (sin errores)
- [ ] Logs muestran OTIF% y Revenue at Risk
- [ ] Dimensiones en cach√© (lookups creados)
- [ ] Hechos insertados sin FKs faltantes
- [ ] Staging marcado como `is_processed = TRUE`

---

**Autor:** Data Engineering Team | Torre Control  
**√öltima Actualizaci√≥n:** 4 Feb 2026  
**Estado:** ‚úÖ Production Ready
