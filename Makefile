.PHONY: help setup init start load transform validate powerbi stop clean all

PYTHON := python3
VENV := .venv
PIP := $(VENV)/bin/pip
PYTHON_VENV := $(VENV)/bin/python
DOCKER_COMPOSE := docker-compose -f config/docker-compose.yml
DB_PORT := 5433
DB_NAME := supply_chain_dw
DB_USER := admin
DB_PASS := adminpassword

# ============================================================================
# COMANDOS PRINCIPALES
# ============================================================================

help: ## ğŸ“– Mostrar ayuda
	@echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
	@echo "  ğŸ¢ TORRE CONTROL - Pipeline de EjecuciÃ³n"
	@echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
	@echo ""
	@echo "Comandos disponibles:"
	@grep -E '^[a-zA-Z_-]+:.*?## .*$$' $(MAKEFILE_LIST) | \
		awk 'BEGIN {FS = ":.*?## "}; {printf "  \033[36m%-18s\033[0m %s\n", $$1, $$2}'
	@echo ""
	@echo "ğŸ¯ Quick Start: make all"
	@echo ""

setup: ## ğŸ”§ InstalaciÃ³n inicial (Python venv + dependencias)
	@echo "ğŸ”§ Creando entorno virtual..."
	$(PYTHON) -m venv $(VENV)
	@echo "ğŸ“¦ Instalando dependencias..."
	$(PIP) install --upgrade pip
	$(PIP) install -r requirements.txt
	@echo "âœ… InstalaciÃ³n completada"

init: ## ğŸ“ Crear estructura de carpetas estÃ¡ndar
	@echo "ğŸ“ Creando estructura de directorios..."
	@mkdir -p data/raw data/interim data/processed data/external
	@mkdir -p notebooks tests logs
	@touch data/.gitkeep notebooks/.gitkeep tests/__init__.py logs/.gitkeep
	@touch src/__init__.py src/etl/__init__.py
	@echo "âœ… Estructura creada"

start: ## ğŸ³ Iniciar infraestructura (Docker PostgreSQL + PgAdmin)
	@echo "ğŸ³ Iniciando contenedores Docker..."
	$(DOCKER_COMPOSE) up -d
	@echo "â³ Esperando PostgreSQL..."
	@sleep 10
	@until docker exec supply_chain_db pg_isready -U $(DB_USER) >/dev/null 2>&1; do \
		sleep 2; \
	done
	@echo "âœ… PostgreSQL listo en localhost:$(DB_PORT)"
	@echo "âœ… PgAdmin: http://localhost:5050"
	@echo "   Email: admin@dataco.com"
	@echo "   Password: $(DB_PASS)"

schema: start ## ğŸ“ Crear schema de Data Warehouse
	@echo "ğŸ“ Ejecutando DDL..."
	@docker exec -i supply_chain_db psql -U $(DB_USER) -d $(DB_NAME) \
		< sql/ddl/01_schema_base.sql
	@echo "âœ… Schema creado: dw.dim_*, dw.fact_orders, dw.stg_raw_orders"

load: ## ğŸ“¥ Cargar datos RAW â†’ Staging
	@echo "ğŸ“¥ Ejecutando carga CSV â†’ PostgreSQL..."
	@if [ ! -f "data/raw/DataCoSupplyChainDataset.csv" ]; then \
		echo "âŒ ERROR: data/raw/DataCoSupplyChainDataset.csv no encontrado"; \
		echo "   Descarga el dataset y colÃ³calo en data/raw/"; \
		exit 1; \
	fi
	$(PYTHON_VENV) scripts/load_data.py
	@echo "âœ… Datos cargados a dw.stg_raw_orders"

transform: ## ğŸ”„ Transformar Staging â†’ Star Schema
	@echo "ğŸ”„ Ejecutando transformaciones..."
	@if [ ! -f "scripts/transform_data.py" ]; then \
		echo "âš ï¸  WARNING: scripts/transform_data.py no existe"; \
		echo "   Creando script bÃ¡sico..."; \
		$(MAKE) create-transform; \
	fi
	$(PYTHON_VENV) scripts/transform_data.py
	@echo "âœ… Dimensiones y hechos poblados"

validate: ## âœ… Validar calidad de datos
	@echo "ğŸ” Validando conteos..."
	@docker exec supply_chain_db psql -U $(DB_USER) -d $(DB_NAME) -c \
		"SELECT 'stg_raw_orders' as tabla, COUNT(*) as registros FROM dw.stg_raw_orders \
		UNION ALL SELECT 'dim_customer', COUNT(*) FROM dw.dim_customer \
		UNION ALL SELECT 'dim_geography', COUNT(*) FROM dw.dim_geography \
		UNION ALL SELECT 'dim_product', COUNT(*) FROM dw.dim_product \
		UNION ALL SELECT 'dim_date', COUNT(*) FROM dw.dim_date \
		UNION ALL SELECT 'fact_orders', COUNT(*) FROM dw.fact_orders;"
	@echo ""
	@echo "ğŸ” OTIF por Market:"
	@docker exec supply_chain_db psql -U $(DB_USER) -d $(DB_NAME) -c \
		"SELECT * FROM dw.v_otif_by_market ORDER BY otif_percentage DESC;"

export-csv: ## ğŸ“¤ Exportar datos a CSV para Power BI
	@echo "ğŸ“¤ Exportando tablas..."
	@mkdir -p data/processed
	@docker exec supply_chain_db psql -U $(DB_USER) -d $(DB_NAME) -c \
		"\COPY (SELECT * FROM dw.fact_orders) TO STDOUT CSV HEADER" \
		> data/processed/fact_orders.csv
	@docker exec supply_chain_db psql -U $(DB_USER) -d $(DB_NAME) -c \
		"\COPY (SELECT * FROM dw.dim_customer) TO STDOUT CSV HEADER" \
		> data/processed/dim_customer.csv
	@docker exec supply_chain_db psql -U $(DB_USER) -d $(DB_NAME) -c \
		"\COPY (SELECT * FROM dw.dim_geography) TO STDOUT CSV HEADER" \
		> data/processed/dim_geography.csv
	@docker exec supply_chain_db psql -U $(DB_USER) -d $(DB_NAME) -c \
		"\COPY (SELECT * FROM dw.dim_product) TO STDOUT CSV HEADER" \
		> data/processed/dim_product.csv
	@docker exec supply_chain_db psql -U $(DB_USER) -d $(DB_NAME) -c \
		"\COPY (SELECT * FROM dw.dim_date) TO STDOUT CSV HEADER" \
		> data/processed/dim_date.csv
	@echo "âœ… CSVs en: data/processed/"
	@ls -lh data/processed/*.csv

powerbi-info: ## ğŸ“Š Mostrar informaciÃ³n de conexiÃ³n Power BI
	@echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
	@echo "  ğŸ“Š POWER BI - InformaciÃ³n de ConexiÃ³n"
	@echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
	@echo ""
	@echo "OPCIÃ“N 1: PostgreSQL DirectQuery (RECOMENDADO)"
	@echo "  Servidor: localhost:$(DB_PORT)"
	@echo "  Base de datos: $(DB_NAME)"
	@echo "  Usuario: $(DB_USER)"
	@echo "  Password: $(DB_PASS)"
	@echo "  Tablas: dw.fact_orders, dw.dim_customer, dw.dim_geography, dw.dim_product, dw.dim_date"
	@echo ""
	@echo "OPCIÃ“N 2: CSV Import (DESARROLLO)"
	@echo "  Ejecutar: make export-csv"
	@echo "  Ruta: $$(pwd)/data/processed/"
	@echo "  Archivos: fact_orders.csv, dim_*.csv"
	@echo ""

stop: ## â¹ï¸  Detener contenedores
	@echo "â¹ï¸  Deteniendo contenedores..."
	$(DOCKER_COMPOSE) stop

clean: ## ğŸ§¹ Limpiar contenedores y datos (Â¡CUIDADO!)
	@echo "âš ï¸  Esto eliminarÃ¡ todos los contenedores y datos"
	@read -p "Â¿Continuar? [y/N]: " confirm && [ "$$confirm" = "y" ]
	$(DOCKER_COMPOSE) down -v
	rm -rf data/processed/*.csv
	@echo "âœ… Limpieza completada"

logs: ## ğŸ“‹ Ver logs de PostgreSQL
	$(DOCKER_COMPOSE) logs -f postgres

test: ## ğŸ§ª Ejecutar tests (si existen)
	@if [ -d "tests" ] && [ -f "tests/test_*.py" ]; then \
		$(PYTHON_VENV) -m pytest tests/ -v; \
	else \
		echo "âš ï¸  No hay tests configurados"; \
	fi

all: setup init start schema load transform validate ## ğŸ¯ Pipeline completo
	@echo ""
	@echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
	@echo "  âœ… PIPELINE COMPLETADO"
	@echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
	@echo ""
	@echo "PrÃ³ximos pasos:"
	@echo "  1. Ver datos: make validate"
	@echo "  2. Exportar CSV: make export-csv"
	@echo "  3. Conectar Power BI: make powerbi-info"
	@echo ""

# ============================================================================
# HELPERS
# ============================================================================

create-transform: ## ğŸ”§ Crear script transform_data.py bÃ¡sico
	@echo "Creando scripts/transform_data.py..."
	@echo '#!/usr/bin/env python3' > scripts/transform_data.py
	@echo '"""Torre Control - ETL Transformation"""' >> scripts/transform_data.py
	@echo 'print("âš ï¸  Script de transformaciÃ³n pendiente de implementar")' >> scripts/transform_data.py
	@echo 'print("Ver documentaciÃ³n para crear populate_dim_* functions")' >> scripts/transform_data.py
	@chmod +x scripts/transform_data.py
	@echo "âœ… scripts/transform_data.py creado"